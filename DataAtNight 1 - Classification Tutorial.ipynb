{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style=\"text-align: center; margin: 0 auto\">\n",
      "    <font style=\"font-family: Bebas Neue; font-size: 36px\"><br>Data At Night #1<br><br>February 8, 2015<br><br>Princeton University<br><br></font>\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Welcome to DataAtNight #1, and to the Princeton Data Science Club! We're thrilled to have you here. If you're new to data science and machine learning, we've prepared a small iPython notebook to get you started with some basic classification.\n",
      "\n",
      "The dataset we've provided describes various features of abalone (shellfish) such as sex, length, diameter, and so forth. In this tutorial, the idea is that, given values of these features (e.g. shellfish has a length of 0.45), we can build a model to predict whether a given abalone is M (male), F (female), or I (infant). Note that we have split the data into two small datasets (80%/20% ratio), a \"train\" dataset to fit and train our model, and a \"test\" dataset to see how our model actually did. First, we'll fit a simple Logistic Regression classifier on the train dataset to tell whether an abalone is M/F/I. Then, we'll try our Logistic Regression model out on our testing data to see how accurately it identifies unseen data.\n",
      "\n",
      "We'll be using the Pandas package in Python which has fast, easy capabilities for manipulating tables of data (or \"dataframes\"; it's basically R data structures in Python). We'll also be using Scikit-Learn, one of the most popular off-the-shelf Python packages for machine learning.\n",
      "\n",
      "Credits to Daway Chou-Ren for the data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, read in the data. Before we can get to the modeling, we need to do some data preprocessing. We read everything into a dataframe (Pandas), which you can think of as a big spreadsheet for the data. Then we separate column 0 (the sex M/F/I, which we want to predict) from the rest of the data (features such as length, rings, etc.) and put it into a NumPy array, specifying that it is of the \"str\" (string) datatype. For this dataset, this preprocessing is necessary. In other datasets, you may have to do much more, or much less."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Read in training, testing data\n",
      "train_data = pd.read_csv(\"abalone_train.txt\", header=None)\n",
      "train_data.columns = [\"Sex\", \"Length\", \"Diameter\", \"Height\", \"Whole weight\", \n",
      "                      \"Shucked weight\", \"Viscera weight\", \"Shell weight\", \"Rings\"]\n",
      "test_data = pd.read_csv(\"abalone_test.txt\", header=None)\n",
      "test_data.columns = [\"Sex\", \"Length\", \"Diameter\", \"Height\", \"Whole weight\", \n",
      "                      \"Shucked weight\", \"Viscera weight\", \"Shell weight\", \"Rings\"]\n",
      "\n",
      "# Sort into x (features) and y (labels) for both train and test data\n",
      "y_train = np.asarray(train_data['Sex']).astype(str)\n",
      "x_train = train_data[train_data.columns[1:]]\n",
      "y_test = np.asarray(test_data['Sex']).astype(str)\n",
      "x_test = test_data[test_data.columns[1:]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Logistic Regression is a basic kind of machine learning model that takes in a linear combination of the features (for example, 2 * length + 3 * height), and then transforms it with the logistic function so that it falls in the interval $[0,1]$ for two labels, say \"M\" and F\". The formal math equation is:\n",
      "<br>\n",
      "<br style=\"text-align: center; margin: 0 auto\">\n",
      "$F(x)=\\frac{1}{1+e^(-B_0+B_1*X_1+B_2*X_2+...)}$\n",
      "<br>\n",
      "<br>\n",
      "Of course, in our example, we use 3 labels instead of 2. The generalization of Logistic Regression to $N$ labels is called Multinomial Logistic Regression.\n",
      "<br>\n",
      "First, we fit the classifier to our training dataset: the features (x_train), and the labels (y_train)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Train a classifier\n",
      "logit = LogisticRegression()\n",
      "logit.fit(x_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see the accuracy score of our classifier, within our training dataset:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_predicted = np.asarray(logit.predict(x_train).astype(str))\n",
      "accuracy_score(y_train, train_predicted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "0.54714157437892852"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, let's see how the logistic regression model does on the testing data. Note that we *do not* train (fit) the model to the testing data; that's the machine learning equivalent of cheating."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_predicted = np.asarray(logit.predict(x_test).astype(str))\n",
      "accuracy_score(y_test, test_predicted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "0.56818181818181823"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Intuitively, note that random guessing, for 3 labels, would probably give you just 1 out of 3 correct (.33 accuracy). So as a sanity check Logistic Regression is better than random labels. Can we do better?\n",
      "<br>\n",
      "<br>\n",
      "Let's run a tool from Scikit-Learn called GridSearchCV (for Grid Search and Cross Validation). Grid search searches exhaustively over possible parameters for a model until it finds the best one. Cross Validation is a way to conduct model testing within the training set (for more detail: \n",
      "<a>en.wikipedia.org/wiki/Cross-validation_(statistics)</a>). This prevents the phenomenon called overfitting, in which your model does really well on the training set, but does not generalizing well to the testing set. It's like memorizing practice test answers; you score really well on the practice tests, but are ill-prepared to do well on (unseen) new tests.\n",
      "<br>\n",
      "<br>\n",
      "If you look at the documentation for Scikit-Learn's Logistic Regression (<a>http://scikit-learn.org/0.15/modules/generated/sklearn.grid_search.GridSearchCV.html</a>), one of the important parameters (regularization) is C. So let's run GridSearchCV to optimize that parameter, and train it on our training set data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import grid_search\n",
      "parameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
      "logit_gs = grid_search.GridSearchCV(LogisticRegression(), parameters)\n",
      "logit_gs.fit(x_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "GridSearchCV(cv=None,\n",
        "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
        "       param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
        "       verbose=0)"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, let's run it on the test set and see if we can beat our 56% accuracy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_predicted_gs = np.asarray(logit_gs.predict(x_test).astype(str))\n",
      "accuracy_score(y_test, test_predicted_gs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "0.57535885167464118"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We've gained a slight increase in accuracy to 57%. There's much more we could do: select the best subset of the features, choose other values for the parameters, choose a different model (SVMs, RandomForests), and so forth.\n",
      "<br>\n",
      "<br>\n",
      "Resources:\n",
      "<br>\n",
      "<br>\n",
      "Scikit-Learn: <a>http://scikit-learn.org/stable/index.html</a>\n",
      "<br>\n",
      "Kaggle, a site with machine learning competitions: <a>http://www.kaggle.com/</a>\n",
      "<br>\n",
      "DataTau, Hacker News for Data Science: <a>http://www.datatau.com/</a>\n",
      "<br>\n",
      "<br>\n",
      "Brought to you by Princeton Data Science. <a>princeton.edu/~datasci</a>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}